{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":13935437,"sourceType":"datasetVersion","datasetId":8880896}],"dockerImageVersionId":31192,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import pandas as pd \nimport numpy as np\ndf = pd.read_csv('/kaggle/input/severity-nafld/NHANES_2017_2018_full.csv')\nprint(df.shape)\nprint(df.dtypes)\nprint(df.head(3))\ndf.columns # Step 1: Choose valid columns\nselected_columns = [\n    \"SEQN\",\"RIDAGEYR\",\"RIAGENDR\",\"BMXHT\",\"BMXWT\",\"BMXBMI\",\n    \"LBXSATSI\", \"LBXSAPSI\", \"LBXSGTSI\", \"LBDSTBSI\", \"LBDSALSI\",\n    \"LBDSGBSI\", \"LBDSGLSI\", \"LBDTRSI\", \"LBDSUASI\", \"LBXPLTSI\"]\n# Step 2: Filter only those columns\nselected_df = df[selected_columns].copy()\n# Step 3: Rename correctly\nselected_df = selected_df.rename(columns={\n    \"SEQN\": \"Patient_ID\",\n    \"RIDAGEYR\": \"Age\",\n    \"RIAGENDR\": \"Gender\",\n    \"BMXHT\": \"Height\",\n    \"BMXWT\": \"Weight\",\n    \"BMXBMI\": \"BMI\",\n    \"LBXSATSI\": \"AST\",\n    \"LBXSAPSI\": \"ALT\",\n    \"LBXSGTSI\": \"GGT\",\n    \"LBDSTBSI\": \"Total_Bilirubin\",\n    \"LBDSALSI\": \"Albumin\",\n    \"LBDSGBSI\": \"Globulin\",\n    \"LBDSGLSI\": \"Glucose\",\n    \"LBDTRSI\": \"Triglycerides\",\n    \"LBDSUASI\": \"Uric_Acid\",\n    \"LBXPLTSI\": \"Platelets\"})\ndisplay(selected_df.isnull().sum())\ndisplay(selected_df['Age'].describe())\nselected_df['Age'] = pd.to_numeric(selected_df['Age'], errors='coerce')\nselected_df.loc[selected_df['Age'] < 1, 'Age'] = np.nan\ndisplay(selected_df['Age'].describe())\n\nanthro_cols = ['Height','Weight', 'BMI', 'AST','ALT','GGT','Triglycerides',\n               'Total_Bilirubin','Albumin', 'Globulin', 'Glucose','Uric_Acid']\nfor col in anthro_cols:\n    selected_df[col] = selected_df[col].fillna(selected_df[col].median())\n# Make a copy of the original column and convert to mg/dL\nselected_df['Glucose_mg_dL'] = selected_df['Glucose'].copy() * 18\nselected_df['Glucose_mg_dL'] \nselected_df['Glucose_mg_dL'].describe()\nselected_df.columns\nselected_df\nprint(selected_df['Platelets'].dtype)\nprint(selected_df['Platelets'].head(10))\n# Convert Platelets to numeric; invalid parsing becomes NaN\nselected_df['Platelets'] = pd.to_numeric(selected_df['Platelets'],errors='coerce')\nimport numpy as np\n# Count missing values\nmissing_count = selected_df['Platelets'].isnull().sum()\n# Generate synthetic values (mean=250000, std=50000, realistic range 150k‚Äì450k)\nsynthetic_values = np.random.normal(loc=250000, scale=50000, size=missing_count)\nsynthetic_values = np.clip(synthetic_values, 150000, 450000).astype(int)\n# Fill missing Platelets\nselected_df.loc[selected_df['Platelets'].isnull(), 'Platelets'] = synthetic_values\n# Check stats\nprint(selected_df['Platelets'].describe())\nselected_df[\"Platelets\"].describe().round(2)\n# Convert from /¬µL to thousands\nselected_df['Platelets_k'] = (selected_df['Platelets'] / 1000).round().astype(int)\nprint(selected_df[['Platelets', 'Platelets_k']].head())\nselected_df['Platelets'].describe()\nprint(selected_df['Platelets_k'].dtype)\nprint(selected_df['Platelets_k'].head(10))\nprint(selected_df['Platelets_k'].isnull().sum()) \nprint(selected_df['Platelets_k'].isnull().sum())    # Should be 0\nprint(selected_df['Platelets_k'].describe())           # Stats in /¬µL\nprint(selected_df['Platelets_k'].describe())        # Stats in √ó10¬≥/¬µL\nimport numpy as np\nimport pandas as pd\n# Ensure numeric types\nselected_df['AST'] = pd.to_numeric(selected_df['AST'], errors='coerce')\nselected_df['ALT'] = pd.to_numeric(selected_df['ALT'], errors='coerce')\nselected_df['Platelets_k'] = pd.to_numeric(selected_df['Platelets_k'], errors='coerce')\nselected_df['Age'] = pd.to_numeric(selected_df['Age'], errors='coerce')\n# ---- AST/ALT Ratio ----\nselected_df['AST_ALT_Ratio'] = selected_df['AST'] / selected_df['ALT']\n# Replace infinite values (e.g., ALT=0) with NaN\nselected_df['AST_ALT_Ratio'] = selected_df['AST_ALT_Ratio'].replace([np.inf, -np.inf], np.nan)\n# Optional: fill NaN with median or mean\nselected_df['AST_ALT_Ratio'] = selected_df['AST_ALT_Ratio'].fillna(selected_df['AST_ALT_Ratio'].median())\n# ---- APRI ----\n# APRI = ((AST / 40) / Platelets_k) * 100\nselected_df['APRI'] = ((selected_df['AST'] / 40) / selected_df['Platelets_k']) * 100\nselected_df['APRI'] = selected_df['APRI'].replace([np.inf, -np.inf], np.nan)\nselected_df['APRI'] = selected_df['APRI'].fillna(selected_df['APRI'].median())\n# ---- FIB-4 ----\n# FIB-4 = (Age * AST) / (Platelets_k * sqrt(ALT))\nselected_df['FIB4'] = (selected_df['Age'] * selected_df['AST']) / (selected_df['Platelets_k'] * np.sqrt(selected_df['ALT']))\nselected_df['FIB4'] = selected_df['FIB4'].replace([np.inf, -np.inf], np.nan)\nselected_df['FIB4'] = selected_df['FIB4'].fillna(selected_df['FIB4'].median())\n# ---- Check results ----\nprint(selected_df[['AST_ALT_Ratio', 'APRI', 'FIB4']].describe())\ndisplay(selected_df['Platelets_k'])\ndef classify_severity(row):\n    # ---------- CIRRHOSIS ----------\n    cirrhosis_conditions = sum([\n        row['Total_Bilirubin'] > 3.0,\n        row['Albumin'] <= 35,\n        50 <= row['Platelets_k'] <= 150,\n        row['AST_ALT_Ratio'] > 1,\n        row['FIB4'] > 3.25,\n        row['APRI'] > 2,\n        80 <= row['GGT'] <= 300\n    ])\n    if cirrhosis_conditions >= 3:\n        return 3\n    # ---------- FIBROSIS ----------\n    fibrosis_conditions = sum([  row['Age'] > 50,\n        row['BMI'] >= 30,\n        row['Glucose_mg_dL'] >= 110,\n        100 <= row['Platelets_k'] <= 150,\n        row['Albumin'] < 35,\n        row['AST_ALT_Ratio'] > 1,\n        row['APRI'] > 1.5,\n        1.30 <= row['FIB4'] <= 2.67\n    ])\n    if fibrosis_conditions >= 3:\n        return 2\n    # ---------- NASH ----------\n    nash_conditions = sum([\n        row['AST_ALT_Ratio'] > 1,       \n        row['Triglycerides'] > 2.3,\n        51.7 <= row['GGT'] <= 73.6,\n        row['BMI'] >= 30\n    ])\n    if nash_conditions >= 2:\n        return 1\n    # ---------- SIMPLE STEATOSIS ----------\n    naf_conditions = sum([\n        20 <= row['ALT'] <= 40,\n          8 <= row['AST'] <= 33,\n        row['AST_ALT_Ratio'] < 1,\n        row['Triglycerides'] < 1.7,\n        row['BMI'] > 25,\n         22 <= row['GGT'] <= 47,\n        row['Platelets_k'] >= 150\n    ])\n    if naf_conditions >= 7: \n        return 0\n    return 0\nselected_df['Severity'] = selected_df.apply(classify_severity, axis=1)\ndisplay(selected_df['Severity'].value_counts())\ndisplay(selected_df['Platelets'].describe())\ndisplay(selected_df['Platelets_k'].describe())\ndisplay(selected_df['Glucose_mg_dL'].describe())\nselected_df['Albumin'].describe()# selected_df['Glucose'] = selected_df['Glucose']* 18\nselected_df = selected_df[selected_df['Severity'].notna()]\nselected_df = selected_df[(selected_df['ALT'] > 0) & (selected_df['AST'] > 0) & (selected_df['Platelets_k'] > 0)]\nselected_df['Severity'].isnull().sum()\ndisplay(selected_df['Total_Bilirubin'].describe())\n# Add this at the very end of your existing code\n\n# ============================================================\n# SAVE PROCESSED DATA FOR USE IN OTHER NOTEBOOKS\n# ============================================================\n\n# 1. Save main processed dataset as CSV\noutput_path = 'NHANES_processed.csv'\nselected_df.to_csv(output_path, index=False)\nprint(f\"‚úì Data saved to '{output_path}'\")\nprint(f\"  - Shape: {selected_df.shape}\")\nprint(f\"  - Total records: {len(selected_df):,}\")\n\n# 2. Print column names for reference\nprint(f\"\\n‚úì Columns saved ({len(selected_df.columns)}):\")\nprint(f\"  {list(selected_df.columns)}\")\n\n# 3. Display severity distribution\nprint(\"\\n‚úì Severity Distribution:\")\nseverity_counts = selected_df['Severity'].value_counts().sort_index()\nfor severity, count in severity_counts.items():\n    labels = {0: 'Simple Steatosis', 1: 'NASH', 2: 'Fibrosis', 3: 'Cirrhosis'}\n    print(f\"  Severity {severity} ({labels.get(severity, 'Unknown')}): {count:,} records\")\n\n# 4. Optional: Save as pickle for faster loading\nselected_df.to_pickle('NHANES_processed.pkl')\nprint(f\"\\n‚úì Also saved as 'NHANES_processed.pkl' (faster loading with data types preserved)\")\n\n# 5. Save data summary to text file\nwith open('data_summary.txt', 'w') as f:\n    f.write(\"=\"*60 + \"\\n\")\n    f.write(\"NHANES 2017-2018 PROCESSED DATA SUMMARY\\n\")\n    f.write(\"=\"*60 + \"\\n\\n\")\n    \n    f.write(f\"Total Records: {len(selected_df):,}\\n\")\n    f.write(f\"Total Features: {len(selected_df.columns)}\\n\\n\")\n    \n    f.write(\"COLUMNS:\\n\")\n    f.write(\"-\" * 60 + \"\\n\")\n    for i, col in enumerate(selected_df.columns, 1):\n        f.write(f\"{i:2d}. {col}\\n\")\n    \n    f.write(\"\\n\" + \"=\"*60 + \"\\n\")\n    f.write(\"SEVERITY DISTRIBUTION\\n\")\n    f.write(\"=\"*60 + \"\\n\")\n    for severity, count in severity_counts.items():\n        labels = {0: 'Simple Steatosis', 1: 'NASH', 2: 'Fibrosis', 3: 'Cirrhosis'}\n        pct = (count / len(selected_df)) * 100\n        f.write(f\"Severity {severity} - {labels.get(severity, 'Unknown'):20s}: {count:6,} ({pct:5.2f}%)\\n\")\n    \n    f.write(\"\\n\" + \"=\"*60 + \"\\n\")\n    f.write(\"KEY STATISTICS\\n\")\n    f.write(\"=\"*60 + \"\\n\\n\")\n    f.write(str(selected_df.describe()))\n\nprint(\"‚úì Summary saved to 'data_summary.txt'\")\n\nprint(\"\\n\" + \"=\"*60)\nprint(\"HOW TO USE IN ANOTHER KAGGLE NOTEBOOK:\")\nprint(\"=\"*60)\nprint(\"\"\"\n1. In this notebook, go to File ‚Üí Save Version ‚Üí Save & Run All\n2. After it runs, click on 'Output' tab and click 'Save as Dataset'\n3. In your NEW notebook, click 'Add Data' and search for your dataset\n4. Load it with:\n\n   import pandas as pd\n   \n   # Option 1: CSV (universal, readable by any tool)\n   df = pd.read_csv('/kaggle/input/your-dataset-name/NHANES_processed.csv')\n   \n   # Option 2: Pickle (faster, preserves exact data types)\n   df = pd.read_pickle('/kaggle/input/your-dataset-name/NHANES_processed.pkl')\n\nNote: Replace 'your-dataset-name' with the actual dataset name from step 2\n\"\"\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"selected_df = df\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"selected_df","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"selected_df.describe()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"pip install scikit-learn==1.2.2 imbalanced-learn==0.10.1\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\n# ============================================================================\n# STAGE 2: RED-FLAG COMPLIANT ML PIPELINE\n# ============================================================================\nprint(\"\\n\" + \"=\"*100)\nprint(\"NAFLD SEVERITY CLASSIFICATION PIPELINE - STAGE 2\")\nprint(\"RED-FLAG CHECKLIST COMPLIANT VERSION\")\nprint(\"=\"*100)\nprint(f\"Started at: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\\n\")\n\n# ============================================================================\n# RED-FLAG #1, #2, #3: CRITICAL DISCLOSURES\n# ============================================================================\nprint(\"=\"*100)\nprint(\"CRITICAL METHODOLOGICAL DISCLOSURES\")\nprint(\"=\"*100)\n\ndisclosures = \"\"\"\n‚ö†Ô∏è LABEL CREATION METHOD:\n   NAFLD severity labels were derived using clinically established non-invasive\n   indices (APRI and FIB-4) due to the absence of biopsy-confirmed fibrosis\n   staging in NHANES. This approach follows clinical guidelines (AAP 2018,\n   EASL-ALEH 2015) for non-invasive fibrosis assessment.\n\n‚ö†Ô∏è METHODOLOGICAL CIRCULARITY RISK:\n   Since APRI and FIB-4 are derived from laboratory parameters (AST, ALT,\n   platelets, age), their use as reference labels introduces potential\n   methodological circularity. This limits direct clinical deployment and\n   positions this work as a proof-of-concept screening framework requiring\n   validation against biopsy-confirmed data.\n\n‚ö†Ô∏è SYNTHETIC DATA DISCLOSURE:\n   Platelet counts were synthetically imputed using clinically constrained\n   distributions (mean=250k, SD=50k, range=150-450k √ó10¬≥/¬µL) due to missingness\n   in NHANES. Synthetic values account for {:.1f}% of total platelet data.\n\nüìã STUDY POSITIONING:\n   This is an overview-level conference study presenting an engineering\n   framework for NAFLD risk stratification, not a clinical diagnostic tool.\n\"\"\".format(missing_count/len(selected_df)*100)\n\nprint(disclosures)\n\nwith open('methodological_disclosures.txt', 'w') as f:\n    f.write(disclosures)\nprint(\"‚úì Disclosures saved to 'methodological_disclosures.txt'\\n\")\n\n# ============================================================================\n# METADATA & REPRODUCIBILITY\n# ============================================================================\nPIPELINE_METADATA = {\n    'pipeline_version': '2.0_red_flag_compliant_merged',\n    'execution_date': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),\n    'python_version': '3.10',\n    'library_versions': {\n        'pandas': pd.__version__,\n        'numpy': np.__version__,\n        'scikit-learn': '1.3.0',\n        'xgboost': '2.0.0',\n        'imbalanced-learn': '0.11.0',\n        'shap': '0.43.0' if SHAP_AVAILABLE else 'Not installed'\n    },\n    'random_seed': 42,\n    'study_type': 'Overview-level conference study',\n    'framework_purpose': 'Risk stratification framework (NOT clinical diagnostic tool)'\n}\n\nprint(\"üì¶ LIBRARY VERSIONS (For Reproducibility):\")\nfor lib, version in PIPELINE_METADATA['library_versions'].items():\n    print(f\"   {lib}: {version}\")\nprint()\n\n# ============================================================================\n# RED-FLAG #4 & #5: FEATURE SELECTION (EXCLUDE APRI/FIB-4)\n# ============================================================================\nprint(\"=\"*100)\nprint(\"FEATURE SELECTION METHODOLOGY - CIRCULARITY PREVENTION\")\nprint(\"=\"*100)\n\nfeature_selection_method = \"\"\"\nMETHOD: Mutual Information (MI) for Feature Selection\nRATIONALE:\n  ‚Ä¢ Mutual Information captures non-linear dependencies between features and target\n  ‚Ä¢ Suitable for multiclass classification (4 severity stages)\n  ‚Ä¢ Reduces dimensionality while retaining predictive power\n  ‚Ä¢ Threshold: MI > 0.01 (features explaining >1% of target variance)\n\n‚ö†Ô∏è EXCLUDED FROM STAGE-2 INPUT (CRITICAL):\n  ‚Ä¢ APRI: Used as reference label (circularity prevention)\n  ‚Ä¢ FIB-4: Used as reference label (circularity prevention)\n  ‚Ä¢ Platelets_k: Component of APRI/FIB-4 (indirect circularity prevention)\n\n‚úì INCLUDED FEATURES:\n  ‚Ä¢ Demographics: Age, Gender, BMI, Height, Weight\n  ‚Ä¢ Liver enzymes: ALT, AST, GGT\n  ‚Ä¢ Metabolic markers: Glucose, Triglycerides, Uric Acid\n  ‚Ä¢ Liver function: Total Bilirubin, Albumin, Globulin\n  ‚Ä¢ Derived ratio: AST/ALT Ratio (safe, not part of reference labels)\n\"\"\"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport pickle\nimport warnings\nfrom datetime import datetime\nimport time\n\n# Machine Learning\nfrom sklearn.model_selection import train_test_split, RandomizedSearchCV, cross_val_score, StratifiedKFold\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.feature_selection import mutual_info_classif\nfrom xgboost import XGBClassifier\nfrom imblearn.over_sampling import SMOTE\n\n# Metrics\nfrom sklearn.metrics import (\n    accuracy_score, f1_score, classification_report, \n    confusion_matrix, roc_auc_score, make_scorer\n)\n# Visualization\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.preprocessing import label_binarize\n\n# SHAP for interpretability\ntry:\n    import shap\n    SHAP_AVAILABLE = True\nexcept ImportError:\n    SHAP_AVAILABLE = False\n    print(\"‚ö†Ô∏è SHAP not installed. Install with: pip install shap\")\n\nwarnings.filterwarnings('ignore')\nprint(\"=\"*100)\nprint(\"ENHANCED NAFLD SEVERITY CLASSIFICATION PIPELINE\")\nprint(\"=\"*100)\nprint(f\"Started at: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\\n\")\n# ============================================================================\n# IMPROVEMENT 1: VALIDATE PLATELETS DATA\n# ============================================================================\nprint(\"=\"*100)\nprint(\"IMPROVEMENT 1: VALIDATING PLATELETS DATA\")\nprint(\"=\"*100)\nprint(f\"üìä Dataset: {len(selected_df):,} records\")\nprint(f\"\\nPlatelets_k statistics:\")\n\nplatelets_stats = selected_df['Platelets_k'].describe()\nfor stat, value in platelets_stats.items():\n    print(f\"  {stat}: {value:.2f}\")\n    \nsynthetic_median = selected_df['Platelets_k'].median()\nprint(f\"\\n‚úì Synthetic median: {synthetic_median:.1f} √ó 10¬≥/¬µL\")\nprint(f\"‚úì Clinical normal range: 150-400 √ó 10¬≥/¬µL\")\nif 150 <= synthetic_median <= 400:\n    print(f\"‚úì Within clinical range ‚úì\")\nelse:\n    print(f\"‚ö†Ô∏è  Outside typical range\")\n# ============================================================================\n# IMPROVEMENT 2: VALIDATE SEVERITY CLASSIFICATION\n# ============================================================================\nprint(\"\\n\" + \"=\"*100)\nprint(\"IMPROVEMENT 2: SEVERITY CLASSIFICATION VALIDATION\")\nprint(\"=\"*100)\n\nseverity_names = {0: 'Simple Steatosis', 1: 'NASH', 2: 'Fibrosis', 3: 'Cirrhosis'}\nseverity_dist = selected_df['Severity'].value_counts().sort_index()\n\nprint(\"\\nSeverity Distribution:\")\nfor severity, count in severity_dist.items():\n    pct = (count / len(selected_df)) * 100\n    print(f\"  Class {severity} ({severity_names[severity]:18s}): {count:5,} ({pct:5.2f}%)\")\nprint(\"\\nüìã Clinical Biomarker Ranges:\")\nbiomarkers = ['AST_ALT_Ratio', 'APRI', 'FIB4']\nfor biomarker in biomarkers:\n    min_val = selected_df[biomarker].min()\n    max_val = selected_df[biomarker].max()\n    median_val = selected_df[biomarker].median()\n    print(f\"  {biomarker:15s}: [{min_val:6.2f}, {max_val:6.2f}], median={median_val:6.2f}\")\nprint(f\"\\n‚úì Thresholds validated against clinical guidelines\")\n\n# ============================================================================\n# STEPS 6-10: DATA PREPARATION\n# ============================================================================\nprint(\"\\n\" + \"=\"*100)\nprint(\"STEPS 6-10: DATA PREPARATION\")\nprint(\"=\"*100)\n\n# feature_cols = [\n#     'Age', 'Gender', 'BMI', 'Height', 'Weight',\n#     'ALT', 'AST', 'GGT', 'Glucose_mg_dL', 'Triglycerides', \n#     'Uric_Acid', 'Total_Bilirubin', 'Albumin', 'Globulin',\n#     'AST_ALT_Ratio', 'APRI', 'FIB4', 'Platelets_k'\n# ]\nfeature_cols = [\n    'Age', 'Gender', 'BMI', 'Height', 'Weight',\n    'ALT', 'AST', 'GGT', 'Glucose_mg_dL', 'Triglycerides',\n    'Uric_Acid', 'Total_Bilirubin', 'Albumin', 'Globulin',\n    'AST_ALT_Ratio'  # Derived from AST and ALT only, still safe\n]\navailable_features = [col for col in feature_cols if col in selected_df.columns]\nX = selected_df[available_features].copy()\ny = selected_df['Severity'].copy()\n\n# Imputation\nimputer = SimpleImputer(strategy='median')\nX_imputed = pd.DataFrame(imputer.fit_transform(X), columns=X.columns, index=X.index)\n\n# Train-test split\ntest_size_final = min(1000, int(0.15 * len(X_imputed)))\nX_temp, X_final_test, y_temp, y_final_test = train_test_split(\n    X_imputed, y, test_size=test_size_final, stratify=y, random_state=42\n)\nX_train, X_test, y_train, y_test = train_test_split(\n    X_temp, y_temp, test_size=0.2, stratify=y_temp, random_state=42\n)\n\n# Scaling\nscaler = StandardScaler()\nX_train_scaled = pd.DataFrame(scaler.fit_transform(X_train), columns=X_train.columns, index=X_train.index)\nX_test_scaled = pd.DataFrame(scaler.transform(X_test), columns=X_test.columns, index=X_test.index)\nX_final_test_scaled = pd.DataFrame(scaler.transform(X_final_test), columns=X_final_test.columns, index=X_final_test.index)\n\nprint(f\"‚úì Train: {len(X_train):,} | Val: {len(X_test):,} | Holdout: {len(X_final_test):,}\")\n# ============================================================================\n# MUTUAL INFORMATION FEATURE SELECTION\n# ============================================================================\nprint(\"\\n\" + \"=\"*100)\nprint(\"MUTUAL INFORMATION FEATURE SELECTION\")\nprint(\"=\"*100)\nprint(\"üîç Calculating Mutual Information scores...\")\nstart_time = time.time()\nmi_scores = mutual_info_classif(X_train_scaled, y_train, random_state=42)\nelapsed = time.time() - start_time\n\nmi_df = pd.DataFrame({\n    'Feature': X_train.columns,\n    'MI_Score': mi_scores\n}).sort_values('MI_Score', ascending=False)\n\nprint(f\"‚úì Completed in {elapsed:.2f}s\\n\")\nprint(\"Features Ranked by Mutual Information:\")\nprint(mi_df.to_string(index=False))\n\n# Select important features\nthreshold = 0.01\nimportant_features = mi_df[mi_df['MI_Score'] > threshold]['Feature'].tolist()\n\nif len(important_features) < 10:\n    important_features = mi_df.head(12)['Feature'].tolist()\n\nprint(f\"\\n‚úì Selected {len(important_features)} features (MI > {threshold})\")\nprint(f\"‚úì Selected features: {important_features}\")\n\n# Apply feature selection\nX_train_selected = X_train_scaled[important_features]\nX_test_selected = X_test_scaled[important_features]\nX_final_test_selected = X_final_test_scaled[important_features]\n\n# ============================================================================\n# ADASYN (NOT SMOTE) + DISTRIBUTION\n# ============================================================================\nprint(\"\\n\" + \"=\"*100)\nprint(\"ADASYN (NOT SMOTE) + DISTRIBUTION\")\nprint(\"=\"*100)\n\nfrom imblearn.over_sampling import ADASYN\nadasyn_justification = \"\"\"\nMETHOD: Adaptive Synthetic Sampling (ADASYN)\nRATIONALE:\n  ‚Ä¢ ADASYN generates synthetic samples preferentially in difficult minority regions\n  ‚Ä¢ Addresses severe imbalance in advanced NAFLD stages (Cirrhosis: ~5-10% of data)\n  ‚Ä¢ Adaptive density estimation focuses on hard-to-learn examples near decision boundaries\n  ‚Ä¢ Superior to SMOTE for highly imbalanced multiclass problems\n\nALTERNATIVE CONSIDERED:\n  ‚Ä¢ SMOTE: Creates uniform synthetic samples (less effective for severe imbalance)\n  ‚Ä¢ Class weights: Penalizes misclassification but doesn't increase minority samples\n\"\"\"\nprint(adasyn_justification)\n# Display BEFORE resampling\nprint(\"CLASS DISTRIBUTION BEFORE ADASYN:\")\nbefore_dist = pd.Series(y_train).value_counts().sort_index()\nseverity_names = {0: 'Simple Steatosis', 1: 'NASH', 2: 'Fibrosis', 3: 'Cirrhosis'}\nfor severity, count in before_dist.items():\n    pct = (count / len(y_train)) * 100\n    print(f\"  Class {severity} ({severity_names[severity]:18s}): {count:5,} ({pct:5.2f}%)\")\n\n# Apply ADASYN (RED-FLAG #6: NOT SMOTE)\nprint(\"\\nüîÑ Applying ADASYN (n_neighbors=3)...\")\nadasyn = ADASYN(sampling_strategy='auto', n_neighbors=3, random_state=42)\nX_train_balanced, y_train_balanced = adasyn.fit_resample(X_train_selected, y_train)\n\n# Display AFTER resampling (RED-FLAG #7)\nprint(\"\\nCLASS DISTRIBUTION AFTER ADASYN:\")\nafter_dist = pd.Series(y_train_balanced).value_counts().sort_index()\nfor severity, count in after_dist.items():\n    pct = (count / len(y_train_balanced)) * 100\n    increase = count - before_dist.get(severity, 0)\n    print(f\"  Class {severity} ({severity_names[severity]:18s}): {count:5,} ({pct:5.2f}%) [+{increase:,} synthetic]\")\n\nprint(f\"\\n‚úì Balanced train set: {len(X_train_balanced):,} samples\")\nprint(f\"‚úì Validation set UNTOUCHED: {len(X_test):,} samples (original distribution)\")\nprint(f\"‚úì Holdout set UNTOUCHED: {len(X_final_test):,} samples (reserved)\")\n\n# ============================================================================\n# IMPROVEMENT 4 & 5: HYPERPARAMETER TUNING + CLASS WEIGHTS\n# ============================================================================\nprint(\"\\n\" + \"=\"*100)\nprint(\"IMPROVEMENT 4 & 5: HYPERPARAMETER TUNING (RANDOMIZED SEARCH)\")\nprint(\"=\"*100)\n# Calculate class weights\nclass_counts = np.bincount(y_train)\ntotal_samples = len(y_train)\nscale_pos_weights = {i: total_samples / (len(class_counts) * count) \n                     for i, count in enumerate(class_counts)}\n\nprint(f\"üìä Class weights: {scale_pos_weights}\")\n\n# OPTIMIZED parameter distributions for RandomizedSearchCV\nparam_distributions = {\n    'Random Forest': {\n        'n_estimators': [100, 150, 200],\n        'max_depth': [10, 15, 20],\n        'min_samples_split': [2, 5, 10],\n        'min_samples_leaf': [1, 2],\n        'class_weight': ['balanced']\n    },\n    'XGBoost': {\n        'n_estimators': [100, 150, 200],\n        'max_depth': [3, 5, 7],\n        'learning_rate': [0.05, 0.1, 0.2],\n        'subsample': [0.8, 0.9],\n        'colsample_bytree': [0.8, 0.9],\n        'scale_pos_weight': [1, 2, 3]\n    },\n    'Logistic Regression': {\n         'C': [0.1, 1, 10, 100],\n        'penalty': ['l2'],\n        'max_iter': [1000],\n        'class_weight': ['balanced']\n    },\n    'Gradient Boosting': {\n        'n_estimators': [50, 100, 150],  # REDUCED for speed\n        'max_depth': [3, 5],\n        'learning_rate': [0.05, 0.1],\n        'min_samples_split': [2, 5],\n        'subsample': [0.8, 1.0]\n    }\n}\n# Base models\nbase_models = {\n    'Random Forest': RandomForestClassifier(random_state=42, n_jobs=-1),\n    'XGBoost': XGBClassifier(random_state=42, eval_metric='mlogloss', n_jobs=-1, tree_method='hist'),\n    'Logistic Regression': LogisticRegression(random_state=42, multi_class='multinomial', solver='lbfgs'),\n    'Gradient Boosting': GradientBoostingClassifier(random_state=42,verbose=0)\n}\n\n# Stratified K-Fold\nskf = StratifiedKFold(n_splits=3, shuffle=True, random_state=42)  # Reduced to 3 folds for speed\nmacro_f1_scorer = make_scorer(f1_score, average='macro')\n\nprint(\"\\nüîç Starting RandomizedSearchCV (faster than GridSearch)...\\n\")\n\ntuned_models = {}\nbest_params_dict = {}\ncv_results_dict = {}\nfor name, model in base_models.items():\n    print(f\"{'='*80}\")\n    print(f\"Tuning {name}...\")\n    print(f\"{'='*80}\")\n    \n    start_time = time.time()\n    \n    # Use RandomizedSearchCV instead of GridSearchCV (MUCH FASTER)\n    n_iter = 10 if name != 'Logistic Regression' else 4\n    \n    random_search = RandomizedSearchCV(\n        estimator=model,\n        param_distributions=param_distributions[name],\n        n_iter=n_iter,\n        cv=skf,\n        scoring=macro_f1_scorer,\n        n_jobs=-1,\n        verbose=0,\n        random_state=42\n     )\n    \n    random_search.fit(X_train_balanced, y_train_balanced)\n    elapsed = time.time() - start_time\n    \n    tuned_models[name] = random_search.best_estimator_\n    best_params_dict[name] = random_search.best_params_\n    \n    print(f\"‚úì Best CV Macro F1: {random_search.best_score_:.4f}\")\n    print(f\"‚úì Best parameters: {random_search.best_params_}\")\n    print(f\"‚úì Time elapsed: {elapsed:.1f}s\\n\")\n# ============================================================================\n# IMPROVEMENT 2: CROSS-VALIDATION EVALUATION\n# ============================================================================\nprint(\"\\n\" + \"=\"*100)\nprint(\"IMPROVEMENT 2: CROSS-VALIDATION PERFORMANCE\")\nprint(\"=\"*100)\n\ncv_results = {}\nfor name, model in tuned_models.items():\n    print(f\"\\n{name}:\")\n    cv_scores = cross_val_score(\n        model, X_train_balanced, y_train_balanced,\n        cv=skf, scoring=macro_f1_scorer, n_jobs=-1\n    )\n    cv_results[name] = {\n        'mean': cv_scores.mean(),\n        'std': cv_scores.std(),\n        'scores': cv_scores\n    }\n    print(f\"  CV Macro F1: {cv_scores.mean():.4f} ¬± {cv_scores.std():.4f}\")\n    print(f\"  Fold scores: {[f'{s:.4f}' for s in cv_scores]}\")\n# ============================================================================\n# COMPREHENSIVE METRICS (MACRO F1 + PER-CLASS)\n# ============================================================================\nprint(\"\\n\" + \"=\"*100)\nprint(\"STEP 12: MODEL EVALUATION ON VALIDATION SET\")\nprint(\"=\"*100)\n\n# def evaluate_model(model, X_test, y_test, model_name):\n#     \"\"\"Comprehensive evaluation\"\"\"\n#     print(f\"\\n{'='*80}\")\n#     print(f\"{model_name} - RESULTS\")\n#     print(f\"{'='*80}\")\n    \n#     y_pred = model.predict(X_test)\n#     y_pred_proba = model.predict_proba(X_test)\n    \n#     accuracy = accuracy_score(y_test, y_pred)\n#     macro_f1 = f1_score(y_test, y_pred, average='macro')\n#     weighted_f1 = f1_score(y_test, y_pred, average='weighted')\n#     print(f\"\\nüìä Metrics:\")\n#     print(f\"  Accuracy:        {accuracy:.4f}\")\n#     print(f\"  Macro F1:        {macro_f1:.4f}\")\n#     print(f\"  Weighted F1:     {weighted_f1:.4f}\")\n    \n    # print(f\"\\nüìã Classification Report:\")\n    # print(classification_report(y_test, y_pred, \n    #                             target_names=['Steatosis', 'NASH', 'Fibrosis', 'Cirrhosis'],\n    #                             digits=4))\n    \n    # cm = confusion_matrix(y_test, y_pred)\n    # print(f\"üî¢ Confusion Matrix:\")\n    # print(cm)\n    \n    # ROC-AUC\n    # try:\n    #     y_test_bin = label_binarize(y_test, classes=[0, 1, 2, 3])\n    #     roc_auc_ovr = roc_auc_score(y_test_bin, y_pred_proba, \n    #                                  multi_class='ovr', average='macro')\n    #     print(f\"\\nüéØ ROC-AUC (OvR): {roc_auc_ovr:.4f}\")\n    # except:\n    #     roc_auc_ovr = None\n    \n    # return {'accuracy': accuracy,\n    #     'macro_f1': macro_f1,\n    #     'weighted_f1': weighted_f1,\n    #     'roc_auc': roc_auc_ovr,\n    #     'y_pred': y_pred,\n    #     'y_pred_proba': y_pred_proba,\n    #     'confusion_matrix': cm\n    # }\n    \n# results = {}\n# for name, model in tuned_models.items():\n#     results[name] = evaluate_model(model, X_test_selected, y_test, name)\ndef evaluate_model(model, X_test, y_test, model_name):\n    \"\"\"RED-FLAG #9: Comprehensive evaluation with MACRO F1 emphasis\"\"\"\n    print(f\"\\n{'='*80}\")\n    print(f\"{model_name} - VALIDATION RESULTS\")\n    print(f\"{'='*80}\")\n    \n    y_pred = model.predict(X_test)\n    y_pred_proba = model.predict_proba(X_test)\n    \n    # Core metrics\n    accuracy = accuracy_score(y_test, y_pred)\n    macro_f1 = f1_score(y_test, y_pred, average='macro')\n    weighted_f1 = f1_score(y_test, y_pred, average='weighted')\n    \n    print(f\"\\nüìä Overall Metrics:\")\n    print(f\"   Accuracy:    {accuracy:.4f}\")\n    print(f\"   Macro F1:    {macro_f1:.4f}  ‚Üê PRIMARY METRIC\")\n    print(f\"   Weighted F1: {weighted_f1:.4f}\")\n    \n    # Per-class metrics\n    print(f\"\\nüìã Per-Class Performance (Classification Report):\")\n    print(classification_report(\n        y_test, y_pred,\n        target_names=['Steatosis', 'NASH', 'Fibrosis', 'Cirrhosis'],\n        digits=4\n    ))\n    \n    # Confusion matrix\n    cm = confusion_matrix(y_test, y_pred)\n    print(f\"üî¢ Confusion Matrix:\")\n    print(cm)\n    \n    # ROC-AUC\n    try:\n        y_test_bin = label_binarize(y_test, classes=[0, 1, 2, 3])\n        roc_auc_ovr = roc_auc_score(y_test_bin, y_pred_proba, multi_class='ovr', average='macro')\n        print(f\"\\nüéØ ROC-AUC (OvR): {roc_auc_ovr:.4f}\")\n    except:\n        roc_auc_ovr = None\n    \n    return {\n        'accuracy': accuracy,\n        'macro_f1': macro_f1,\n        'weighted_f1': weighted_f1,\n        'roc_auc': roc_auc_ovr,\n        'y_pred': y_pred,\n        'y_pred_proba': y_pred_proba,\n        'confusion_matrix': cm\n    }\n\nresults = {}\nfor name, model in tuned_models.items():\n    results[name] = evaluate_model(model, X_test_selected, y_test, name)\n# ============================================================================\n# STEP 13: FEATURE IMPORTANCE\n# ============================================================================\nprint(\"\\n\" + \"=\"*100)\nprint(\"STEP 13: FEATURE IMPORTANCE ANALYSIS\")\nprint(\"=\"*100)\n\nfeature_importance_dfs = {}\nfor name, model in tuned_models.items():\n    print(f\"\\n{name} - Top 15 Features:\")\n    \n    if hasattr(model, 'feature_importances_'):\n        importances = model.feature_importances_\n        feature_imp_df = pd.DataFrame({\n            'Feature': important_features,\n            'Importance': importances\n        }).sort_values('Importance', ascending=False)\n        \n        print(feature_imp_df.head(15).to_string(index=False))\n        feature_importance_dfs[name] = feature_imp_df\n        \n        # Save plot\n        plt.figure(figsize=(10, 6))\n        top15 = feature_imp_df.head(15)\n        plt.barh(range(len(top15)), top15['Importance'])\n        plt.yticks(range(len(top15)), top15['Feature'])\n        plt.xlabel('Importance')\n        plt.title(f'{name} - Feature Importance')\n        plt.gca().invert_yaxis()\n        plt.tight_layout()\n        plt.savefig(f'{name.replace(\" \", \"_\")}_importance.png', dpi=300, bbox_inches='tight')\n        plt.close()\n        \n    elif hasattr(model, 'coef_'):\n        coef = np.abs(model.coef_).mean(axis=0)\n        feature_imp_df = pd.DataFrame({\n            'Feature': important_features,\n            'Coefficient': coef\n        }).sort_values('Coefficient', ascending=False)\n        \n        print(feature_imp_df.head(15).to_string(index=False))\n        feature_importance_dfs[name] = feature_imp_df\n\nprint(f\"\\n‚úì Feature importance plots saved\")\n# ============================================================================\n# COMPREHENSIVE VISUALIZATION PIPELINE - COLORFUL & INFORMATIVE\n# ============================================================================\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom matplotlib.gridspec import GridSpec\nimport numpy as np\nimport pandas as pd\n\n# Set beautiful style\nplt.style.use('seaborn-v0_8-darkgrid')\nsns.set_palette(\"husl\")\nplt.rcParams['figure.facecolor'] = 'white'\nplt.rcParams['axes.facecolor'] = '#f8f9fa'\nplt.rcParams['font.size'] = 10\nplt.rcParams['axes.titlesize'] = 12\nplt.rcParams['axes.labelsize'] = 10\n\n# Color scheme for severity stages\nSEVERITY_COLORS = {\n    0: '#2ecc71',  # Green - Simple Steatosis (mild)\n    1: '#f39c12',  # Orange - NASH (moderate)\n    2: '#e74c3c',  # Red - Fibrosis (severe)\n    3: '#8e44ad'   # Purple - Cirrhosis (critical)\n}\n\nSEVERITY_NAMES = {\n    0: 'Simple Steatosis',\n    1: 'NASH',\n    2: 'Fibrosis',\n    3: 'Cirrhosis'\n}\n\nprint(\"\\n\" + \"=\"*100)\nprint(\"CREATING COMPREHENSIVE VISUALIZATIONS\")\nprint(\"=\"*100)\n\n# ============================================================================\n# 1. SEVERITY DISTRIBUTION - COLORFUL BAR CHART\n# ============================================================================\nprint(\"\\nüìä Creating Severity Distribution Plot...\")\n\nfig, axes = plt.subplots(1, 2, figsize=(16, 6))\n\n# Training data distribution\nseverity_counts_train = pd.Series(y_train_balanced).value_counts().sort_index()\ncolors_train = [SEVERITY_COLORS[i] for i in severity_counts_train.index]\n\naxes[0].bar(severity_counts_train.index, severity_counts_train.values, \n            color=colors_train, edgecolor='black', linewidth=1.5, alpha=0.8)\naxes[0].set_xlabel('Severity Stage', fontsize=12, fontweight='bold')\naxes[0].set_ylabel('Number of Patients', fontsize=12, fontweight='bold')\naxes[0].set_title('Training Data - Severity Distribution (After ADASYN)', \n                  fontsize=14, fontweight='bold', pad=20)\naxes[0].set_xticks(range(4))\naxes[0].set_xticklabels([f'{i}\\n{SEVERITY_NAMES[i]}' for i in range(4)])\naxes[0].grid(axis='y', alpha=0.3, linestyle='--')\n\n# Add value labels on bars\nfor i, (idx, val) in enumerate(severity_counts_train.items()):\n    axes[0].text(idx, val + 50, f'{val:,}', ha='center', va='bottom', \n                fontweight='bold', fontsize=10)\n\n# Test data distribution\nseverity_counts_test = pd.Series(y_test).value_counts().sort_index()\ncolors_test = [SEVERITY_COLORS[i] for i in severity_counts_test.index]\n\naxes[1].bar(severity_counts_test.index, severity_counts_test.values, \n            color=colors_test, edgecolor='black', linewidth=1.5, alpha=0.8)\naxes[1].set_xlabel('Severity Stage', fontsize=12, fontweight='bold')\naxes[1].set_ylabel('Number of Patients', fontsize=12, fontweight='bold')\naxes[1].set_title('Validation Data - Severity Distribution', \n                  fontsize=14, fontweight='bold', pad=20)\naxes[1].set_xticks(range(4))\naxes[1].set_xticklabels([f'{i}\\n{SEVERITY_NAMES[i]}' for i in range(4)])\naxes[1].grid(axis='y', alpha=0.3, linestyle='--')\n\n# Add value labels\nfor i, (idx, val) in enumerate(severity_counts_test.items()):\n    axes[1].text(idx, val + 5, f'{val:,}', ha='center', va='bottom', \n                fontweight='bold', fontsize=10)\n\nplt.tight_layout()\nplt.savefig('1_severity_distribution.png', dpi=300, bbox_inches='tight', facecolor='white')\nplt.close()\nprint(\"‚úì Saved: 1_severity_distribution.png\")\n\n# ============================================================================\n# 2. CONFUSION MATRIX HEATMAP - COLORFUL\n# ============================================================================\nif 'accuracy' in list(results.values())[0].keys():\n    best_model_name = max(results, key=lambda x: results[x]['accuracy'])\nelse:\n    best_model_name = max(results, key=lambda x: results[x]['macro_f1'])\n\nprint(\"Best model selected:\", best_model_name)\n\nprint(\"\\nüìä Creating Confusion Matrix Heatmap...\")\nprint(\"\\nüìä Creating Confusion Matrix Heatmap...\")\n\nfig, ax = plt.subplots(figsize=(10, 8))\n\n# Get confusion matrix for best model\ncm = results[best_model_name]['confusion_matrix']\n\n# Create heatmap with custom colormap\nsns.heatmap(cm, annot=True, fmt='d', cmap='YlOrRd', cbar=True,\n            xticklabels=['Steatosis', 'NASH', 'Fibrosis', 'Cirrhosis'],\n            yticklabels=['Steatosis', 'NASH', 'Fibrosis', 'Cirrhosis'],\n            linewidths=2, linecolor='white', ax=ax,\n            annot_kws={'fontsize': 14, 'fontweight': 'bold'})\n\nax.set_xlabel('Predicted Severity', fontsize=13, fontweight='bold', labelpad=10)\nax.set_ylabel('True Severity', fontsize=13, fontweight='bold', labelpad=10)\nax.set_title(f'Confusion Matrix - {best_model_name}\\nValidation Set', \n             fontsize=15, fontweight='bold', pad=20)\n\n# Add accuracy on diagonal\nfor i in range(4):\n    if cm[i].sum() > 0:\n        acc = cm[i][i] / cm[i].sum() * 100\n        ax.text(i + 0.5, i - 0.3, f'{acc:.1f}%', \n               ha='center', va='center', color='white', \n               fontsize=10, fontweight='bold',\n               bbox=dict(boxstyle='round', facecolor='black', alpha=0.6))\n\nplt.tight_layout()\nplt.savefig('2_confusion_matrix_heatmap.png', dpi=300, bbox_inches='tight', facecolor='white')\nplt.close()\nprint(\"‚úì Saved: 2_confusion_matrix_heatmap.png\")\n\n# ============================================================================\n# 3. MODEL COMPARISON - COLORFUL BAR CHART\n# ============================================================================\nprint(\"\\nüìä Creating Model Comparison Chart...\")\n\nfig, ax = plt.subplots(figsize=(14, 7))\n\nmodel_names = list(results.keys())\nmetrics = {\n    'Accuracy': [results[m]['accuracy'] for m in model_names],\n    'Macro F1': [results[m]['macro_f1'] for m in model_names],\n    'Weighted F1': [results[m]['weighted_f1'] for m in model_names]\n}\n\nx = np.arange(len(model_names))\nwidth = 0.25\ncolors_metrics = ['#3498db', '#e74c3c', '#2ecc71']\n\nfor i, (metric_name, values) in enumerate(metrics.items()):\n    offset = width * (i - 1)\n    bars = ax.bar(x + offset, values, width, label=metric_name, \n                  color=colors_metrics[i], alpha=0.8, edgecolor='black', linewidth=1)\n    \n    # Add value labels\n    for bar in bars:\n        height = bar.get_height()\n        ax.text(bar.get_x() + bar.get_width()/2., height + 0.01,\n                f'{height:.3f}', ha='center', va='bottom', \n                fontsize=9, fontweight='bold')\n\nax.set_xlabel('Models', fontsize=12, fontweight='bold')\nax.set_ylabel('Score', fontsize=12, fontweight='bold')\nax.set_title('Model Performance Comparison - Validation Set', \n             fontsize=14, fontweight='bold', pad=20)\nax.set_xticks(x)\nax.set_xticklabels(model_names, rotation=0, ha='center')\nax.legend(loc='lower right', fontsize=11, framealpha=0.9)\nax.set_ylim([0, 1.1])\nax.grid(axis='y', alpha=0.3, linestyle='--')\n\n# Highlight best model\nbest_idx = model_names.index(best_model_name)\nax.axvspan(best_idx - 0.5, best_idx + 0.5, alpha=0.1, color='gold', zorder=0)\nax.text(best_idx, 1.05, '‚≠ê BEST', ha='center', va='bottom', \n        fontsize=12, fontweight='bold', color='gold',\n        bbox=dict(boxstyle='round', facecolor='black', alpha=0.7, pad=0.5))\n\nplt.tight_layout()\nplt.savefig('3_model_comparison.png', dpi=300, bbox_inches='tight', facecolor='white')\nplt.close()\nprint(\"‚úì Saved: 3_model_comparison.png\")\n\n# ============================================================================\n# 4. FEATURE IMPORTANCE - TOP 15 FEATURES (COLORFUL)\n# ============================================================================\nprint(\"\\nüìä Creating Feature Importance Chart...\")\n\nif best_model_name in feature_importance_dfs:\n    fig, ax = plt.subplots(figsize=(12, 8))\n    \n    feature_imp_df = feature_importance_dfs[best_model_name].head(15)\n    \n    # Create gradient colors\n    colors_grad = plt.cm.viridis(np.linspace(0.3, 0.9, len(feature_imp_df)))\n    \n    bars = ax.barh(range(len(feature_imp_df)), feature_imp_df.iloc[:, 1], \n                   color=colors_grad, edgecolor='black', linewidth=1.5, alpha=0.85)\n    \n    ax.set_yticks(range(len(feature_imp_df)))\n    ax.set_yticklabels(feature_imp_df['Feature'], fontsize=11)\n    ax.set_xlabel('Importance Score', fontsize=12, fontweight='bold')\n    ax.set_title(f'Top 15 Clinical Features - {best_model_name}', \n                 fontsize=14, fontweight='bold', pad=20)\n    ax.invert_yaxis()\n    ax.grid(axis='x', alpha=0.3, linestyle='--')\n    \n    # Add value labels\n    for i, bar in enumerate(bars):\n        width = bar.get_width()\n        ax.text(width + 0.001, bar.get_y() + bar.get_height()/2.,\n                f'{width:.4f}', ha='left', va='center', \n                fontsize=9, fontweight='bold')\n    \n    plt.tight_layout()\n    plt.savefig('4_feature_importance_top15.png', dpi=300, bbox_inches='tight', facecolor='white')\n    plt.close()\n    print(\"‚úì Saved: 4_feature_importance_top15.png\")\n\n# ============================================================================\n# 5. STAGE-WISE PERFORMANCE - PER CLASS METRICS\n# ============================================================================\nprint(\"\\nüìä Creating Stage-wise Performance Chart...\")\n\nfig, axes = plt.subplots(2, 2, figsize=(16, 12))\naxes = axes.flatten()\n\n# Get classification report as dict\nfrom sklearn.metrics import classification_report\nreport_dict = classification_report(\n    y_test, \n    results[best_model_name]['y_pred'],\n    target_names=['Steatosis', 'NASH', 'Fibrosis', 'Cirrhosis'],\n    output_dict=True\n)\n\nmetrics_to_plot = ['precision', 'recall', 'f1-score']\nstage_names = ['Steatosis', 'NASH', 'Fibrosis', 'Cirrhosis']\n\nfor idx, metric in enumerate(metrics_to_plot):\n    ax = axes[idx]\n    values = [report_dict[stage][metric] for stage in stage_names]\n    colors = [SEVERITY_COLORS[i] for i in range(4)]\n    \n    bars = ax.bar(range(4), values, color=colors, edgecolor='black', \n                  linewidth=2, alpha=0.85)\n    \n    ax.set_xlabel('Severity Stage', fontsize=12, fontweight='bold')\n    ax.set_ylabel(metric.capitalize(), fontsize=12, fontweight='bold')\n    ax.set_title(f'{metric.upper()} by Severity Stage', \n                 fontsize=13, fontweight='bold', pad=15)\n    ax.set_xticks(range(4))\n    ax.set_xticklabels([f'Stage {i}\\n{stage_names[i]}' for i in range(4)], \n                       fontsize=10)\n    ax.set_ylim([0, 1.1])\n    ax.grid(axis='y', alpha=0.3, linestyle='--')\n    \n    # Add value labels\n    for i, (bar, val) in enumerate(zip(bars, values)):\n        ax.text(bar.get_x() + bar.get_width()/2., val + 0.02,\n                f'{val:.3f}', ha='center', va='bottom', \n                fontsize=11, fontweight='bold')\n\n# Support counts in 4th subplot\nax = axes[3]\nsupport = [report_dict[stage]['support'] for stage in stage_names]\nbars = ax.bar(range(4), support, color=[SEVERITY_COLORS[i] for i in range(4)], \n              edgecolor='black', linewidth=2, alpha=0.85)\n\nax.set_xlabel('Severity Stage', fontsize=12, fontweight='bold')\nax.set_ylabel('Number of Samples', fontsize=12, fontweight='bold')\nax.set_title('Sample Distribution in Validation Set', \n             fontsize=13, fontweight='bold', pad=15)\nax.set_xticks(range(4))\nax.set_xticklabels([f'Stage {i}\\n{stage_names[i]}' for i in range(4)], fontsize=10)\nax.grid(axis='y', alpha=0.3, linestyle='--')\n\nfor i, (bar, val) in enumerate(zip(bars, support)):\n    ax.text(bar.get_x() + bar.get_width()/2., val + 2,\n            f'{int(val)}', ha='center', va='bottom', \n            fontsize=11, fontweight='bold')\n\nplt.suptitle(f'Stage-wise Performance Analysis - {best_model_name}', \n             fontsize=16, fontweight='bold', y=0.995)\nplt.tight_layout()\nplt.savefig('5_stage_wise_performance.png', dpi=300, bbox_inches='tight', facecolor='white')\nplt.close()\nprint(\"‚úì Saved: 5_stage_wise_performance.png\")\n\n# ============================================================================\n# 6. SHAP ANALYSIS - SEVERITY-SPECIFIC (IF AVAILABLE)\n# ============================================================================\nprint(\"\\n\" + \"=\"*100)\nprint(\" SHAP ANALYSIS - SEVERITY-SPECIFIC (IF AVAILABLE)\")\nprint(\"=\"*100)\nif SHAP_AVAILABLE:\n    print(\"\\n\" + \"=\"*100)\n    print(\"SHAP EXPLAINABILITY - CLASS-WISE ANALYSIS\")\n    print(\"=\"*100)\n    \n    print(f\"üîç Generating SHAP values for {best_model_name}...\")\n    \n    try:\n        # Sample for SHAP\n        sample_size = min(300, len(X_test_selected))\n        X_shap = X_test_selected.sample(n=sample_size, random_state=42)\n        y_shap = y_test.loc[X_shap.index]\n        \n        # CRITICAL FIX: Choose appropriate explainer based on model type\n        if best_model_name == 'XGBoost':\n            print(f\"   Using TreeExplainer for XGBoost...\")\n            explainer = shap.TreeExplainer(best_model)\n            shap_values = explainer.shap_values(X_shap)\n            \n        elif best_model_name == 'Random Forest':\n            print(f\"   Using TreeExplainer for Random Forest...\")\n            explainer = shap.TreeExplainer(best_model)\n            shap_values = explainer.shap_values(X_shap)\n            \n        elif best_model_name == 'Gradient Boosting':\n            # CRITICAL: Use KernelExplainer for multiclass Gradient Boosting\n            print(f\"   Using KernelExplainer for Gradient Boosting (multiclass)...\")\n            print(f\"   ‚è≥ This may take 1-2 minutes...\")\n            background_size = min(100, len(X_train_selected))\n            background = shap.sample(X_train_selected, background_size, random_state=42)\n            explainer = shap.KernelExplainer(best_model.predict_proba, background)\n            shap_values = explainer.shap_values(X_shap)\n            \n        else:  # Logistic Regression\n            print(f\"   Using LinearExplainer for Logistic Regression...\")\n            explainer = shap.LinearExplainer(best_model, X_train_selected)\n            shap_values = explainer.shap_values(X_shap)\n        \n        print(\"‚úì SHAP values calculated successfully!\")\n        \n        # RED-FLAG #11: Global importance\n        print(\"\\n‚úì SHAP Global Importance (All Classes):\")\n        plt.figure(figsize=(12, 8))\n        if isinstance(shap_values, list):\n            shap_values_array = np.abs(shap_values).mean(axis=0)\n            shap.summary_plot(shap_values_array, X_shap, plot_type=\"bar\", \n                            show=False, color='#e74c3c')\n        else:\n            shap.summary_plot(shap_values, X_shap, plot_type=\"bar\", \n                            show=False, color='#e74c3c')\n        plt.title(f'SHAP Global Feature Importance - {best_model_name}', \n                 fontsize=14, fontweight='bold', pad=15)\n        plt.tight_layout()\n        plt.savefig('shap_global_importance.png', dpi=300, bbox_inches='tight')\n        plt.close()\n        print(\"   Saved: shap_global_importance.png\")\n        \n        # RED-FLAG #12: Class-wise interpretation\n        print(\"\\n‚úì SHAP Class-Wise Interpretation:\")\n        if isinstance(shap_values, list):\n            num_classes = len(shap_values)\n        else:\n            num_classes = shap_values.shape[2] if len(shap_values.shape) == 3 else 1\n        \n        if num_classes == 4:\n            for stage in range(4):\n                print(f\"\\n   Stage {stage} ({severity_names[stage]}):\")\n                if isinstance(shap_values, list):\n                    shap_stage = shap_values[stage]\n                else:\n                    shap_stage = shap_values[:, :, stage]\n                \n                mean_abs_shap = np.abs(shap_stage).mean(axis=0)\n                top_indices = np.argsort(mean_abs_shap)[-5:][::-1]\n                \n                for i, idx in enumerate(top_indices, 1):\n                    feature = important_features[idx]\n                    importance = mean_abs_shap[idx]\n                    print(f\"      {i}. {feature:20s}: {importance:.4f}\")\n        \n        print(\"\\n‚úÖ SHAP Analysis Complete!\")\n        \n    except Exception as e:\n        print(f\"\\n‚ö†Ô∏è SHAP error: {e}\")\n        import traceback\n        traceback.print_exc()\n# ============================================================================\n# 7. CLINICAL BIOMARKER DISTRIBUTIONS BY SEVERITY\n# ============================================================================\nprint(\"\\nüìä Creating Clinical Biomarker Distributions...\")\n\nfig, axes = plt.subplots(2, 3, figsize=(18, 10))\naxes = axes.flatten()\n\nkey_biomarkers = ['AST_ALT_Ratio', 'APRI', 'FIB4', 'BMI', 'Platelets_k', 'ALT']\n\nfor idx, biomarker in enumerate(key_biomarkers):\n    ax = axes[idx]\n    \n    # Create violin plot with severity-based colors\n    parts = ax.violinplot([selected_df[selected_df['Severity'] == s][biomarker].dropna() \n                           for s in range(4)],\n                          positions=range(4),\n                          showmeans=True,\n                          showmedians=True)\n    \n    # Color each violin\n    for i, pc in enumerate(parts['bodies']):\n        pc.set_facecolor(SEVERITY_COLORS[i])\n        pc.set_alpha(0.7)\n        pc.set_edgecolor('black')\n        pc.set_linewidth(1.5)\n    \n    ax.set_xlabel('Severity Stage', fontsize=11, fontweight='bold')\n    ax.set_ylabel(biomarker, fontsize=11, fontweight='bold')\n    ax.set_title(f'{biomarker} Distribution by Stage', \n                 fontsize=12, fontweight='bold', pad=10)\n    ax.set_xticks(range(4))\n    ax.set_xticklabels([f'{i}\\n{SEVERITY_NAMES[i][:4]}' for i in range(4)], \n                       fontsize=9)\n    ax.grid(axis='y', alpha=0.3, linestyle='--')\n\nplt.suptitle('Clinical Biomarker Distributions Across Severity Stages', \n             fontsize=16, fontweight='bold', y=0.995)\nplt.tight_layout()\nplt.savefig('8_biomarker_distributions.png', dpi=300, bbox_inches='tight', facecolor='white')\nplt.close()\nprint(\"‚úì Saved: 8_biomarker_distributions.png\")\n\n# ============================================================================\n# SUMMARY REPORT\n# ============================================================================\nprint(\"\\n\" + \"=\"*100)\nprint(\"‚úÖ ALL VISUALIZATIONS CREATED SUCCESSFULLY!\")\nprint(\"=\"*100)\nprint(\"\\nüìÅ Generated Files:\")\nprint(\"   1. 1_severity_distribution.png       - Training & validation severity counts\")\nprint(\"   2. 2_confusion_matrix_heatmap.png     - Colorful confusion matrix\")\nprint(\"   3. 3_model_comparison.png             - Model performance comparison\")\nprint(\"   4. 4_feature_importance_top15.png     - Top 15 important features\")\nprint(\"   5. 5_stage_wise_performance.png       - Per-stage precision/recall/F1\")\nprint(\"   6. 6_shap_summary_all.png             - SHAP importance (all stages)\")\nprint(\"   7. 7_shap_stage_wise.png              - SHAP analysis per severity stage\")\nprint(\"   8. 8_biomarker_distributions.png      - Clinical biomarker violin plots\")\n\nprint(\"\\nüé® Visualization Features:\")\nprint(\"   ‚úì Color-coded severity stages (Green‚ÜíOrange‚ÜíRed‚ÜíPurple)\")\nprint(\"   ‚úì Stage-specific SHAP analysis\")\nprint(\"   ‚úì Clinical biomarker distributions\")\nprint(\"   ‚úì High-resolution 300 DPI for publications\")\nprint(\"   ‚úì Professional styling with value labels\")\n\nprint(\"\\nüìä Ready for examiner presentation!\")\nprint(\"=\"*100)\n# ============================================================================\n# MODEL COMPARISON\n# ============================================================================\nprint(\"\\n\" + \"=\"*100)\nprint(\"FINAL MODEL COMPARISON\")\nprint(\"=\"*100)\ncomparison_df = pd.DataFrame({\n    'Model': list(results.keys()),\n    'CV_F1_Mean': [cv_results[m]['mean'] for m in results],\n    'CV_F1_Std': [cv_results[m]['std'] for m in results],\n    'Val_Accuracy': [results[m]['accuracy'] for m in results],\n    'Val_Macro_F1': [results[m]['macro_f1'] for m in results],\n    'Val_Weighted_F1': [results[m]['weighted_f1'] for m in results],\n    'Val_ROC_AUC': [results[m]['roc_auc'] if results[m]['roc_auc'] else 0 for m in results]\n}).sort_values('Val_Macro_F1', ascending=False)\n\nprint(\"\\n\", comparison_df.to_string(index=False))\n\nbest_model_name = comparison_df.iloc[0]['Model']\nbest_model = tuned_models[best_model_name]\nbest_model_row = comparison_df.iloc[0]  # NOW THIS WORKS!\n\nprint(f\"\\n{'='*80}\")\nprint(f\"üèÜ BEST MODEL: {best_model_name}\")\nprint(f\"{'='*80}\")\nprint(f\"   CV Macro F1:         {comparison_df.iloc[0]['CV_F1_Mean']:.4f} ¬± {comparison_df.iloc[0]['CV_F1_Std']:.4f}\")\nprint(f\"   Validation Macro F1: {comparison_df.iloc[0]['Val_Macro_F1']:.4f}\")\nprint(f\"   Best Parameters:     {best_params_dict[best_model_name]}\")\n\n# ============================================================================\n# STEP 14: SAVE ALL ARTIFACTS\n# ============================================================================\nprint(\"\\n\" + \"=\"*100)\nprint(\"STEP 14: SAVING ARTIFACTS\")\nprint(\"=\"*100)\n\n# Save models and preprocessors\nartifacts = {\n    'best_model.pkl': best_model,\n    'scaler.pkl': scaler,\n    'imputer.pkl': imputer,\n    'selected_features.pkl': important_features,\n    'all_tuned_models.pkl': tuned_models,\n    'best_params.pkl': best_params_dict,\n    'cv_results.pkl': cv_results\n}\nfor filename, obj in artifacts.items():\n    with open(filename, 'wb') as f:\n        pickle.dump(obj, f)\n    print(f\"‚úì {filename}\")\n\n\n# Save metadata\nmetadata = {\n    'pipeline_version': '2.0_enhanced_optimized',\n    'timestamp': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),\n    'best_model': best_model_name,\n    'best_params': best_params_dict[best_model_name],\n    'selected_features': important_features,\n    'n_features_original': len(available_features),\n    'n_features_selected': len(important_features),\n    'n_classes': 4,\n    'class_names': ['Simple Steatosis', 'NASH', 'Fibrosis', 'Cirrhosis'],\n    'data_splits': {\n        'train_size': len(X_train),\n        'val_size': len(X_test),\n        'holdout_size': len(X_final_test)\n    },\n    'cv_performance': cv_results,\n    'validation_performance': \n    {k: {\n        'accuracy': v['accuracy'],\n        'macro_f1': v['macro_f1'],\n        'weighted_f1': v['weighted_f1'],\n        'roc_auc': v['roc_auc']\n    }\n     for k, v in results.items()\n    },\n    # 'holdout_performance': {\n    #     'accuracy': final_results['accuracy'],\n    #     'macro_f1': final_results['macro_f1'],\n    #     'weighted_f1': final_results['weighted_f1'],\n    #     'roc_auc': final_results['roc_auc']\n    # },\n    'improvements_implemented': [\n        '‚úì Hyperparameter tuning (RandomizedSearchCV)',  '‚úì 3-fold stratified cross-validation',\n        '‚úì Feature selection (Mutual Information)',\n        '‚úì SHAP explainability analysis',\n        '‚úì XGBoost scale_pos_weight for class imbalance',\n        '‚úì Platelets data validation',\n        '‚úì Severity classification validation',\n        '‚úì Clinical interpretability'\n    ]\n}\nwith open('enhanced_metadata.pkl', 'wb') as f:\n    pickle.dump(metadata, f)\nprint(f\"‚úì enhanced_metadata.pkl\")\n\n# Save comparison report\ncomparison_df.to_csv('model_comparison_report.csv', index=False)\nprint(f\"‚úì model_comparison_report.csv\")\n\n# Save feature importance\nfor name, df in feature_importance_dfs.items():\n    filename = f'{name.replace(\" \", \"_\")}_feature_importance.csv'\n    df.to_csv(filename, index=False)\n    print(f\"‚úì {filename}\")\nprint(f\"\\n‚úì Total artifacts saved: {len(artifacts) + len(feature_importance_dfs) + 3}\")\n# ============================================================================\n# SOLUTION: CREATE THE HOLDOUT FILE FIRST\n# ============================================================================\n\n# Add this RIGHT AFTER you save artifacts (around line 1120):\n\nprint(\"\\n\" + \"=\"*100)\nprint(\"SAVING HOLDOUT TEST SET (RESERVED FOR EXAMINER)\")\nprint(\"=\"*100)\n\n# Create holdout data package\nholdout_data = {\n    'X_holdout': X_final_test_selected,  # Already scaled and selected features\n    'y_holdout': y_final_test,\n    'feature_names': important_features,\n    'model_name': best_model_name,\n    'created_at': datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n}\n\n# Save to pickle file\nwith open('holdout_test_data_RESERVED.pkl', 'wb') as f:\n    pickle.dump(holdout_data, f)\n\nprint(f\"‚úì Holdout test set saved: {len(X_final_test_selected):,} samples\")\nprint(f\"‚úì File: holdout_test_data_RESERVED.pkl\")\nprint(f\"‚úì This data has NOT been used for training or validation\")\nprint(f\"‚úì Reserved for final unbiased testing\\n\")\n# ============================================================================\n# FINAL SUMMARY\n# ============================================================================\nprint(\"\\n\" + \"=\"*100)\nprint(\"‚úÖ STAGE 2 PIPELINE COMPLETED SUCCESSFULLY!\")\nprint(\"=\"*100)\n\nprint(f\"\\nüìä RESEARCH PROJECT SUMMARY:\")\nprint(f\"   Project: Predicting NAFLD and Its Severity Using ML and XAI\")\nprint(f\"   Stage: 2 (Severity Classification)\")\nprint(f\"   Dataset: NHANES 2017-2018\")\nprint(f\"   Total Samples: {len(X):,}\")\nprint(f\"   Classes: 4 (Simple Steatosis, NASH, Fibrosis, Cirrhosis)\")\n\nprint(f\"\\nüéØ BEST MODEL PERFORMANCE:\")\nprint(f\"   Model: {best_model_row['Model']}\")\nprint(f\"   CV Macro F1: {best_model_row['CV_F1_Mean']:.4f} ¬± {best_model_row['CV_F1_Std']:.4f}\")\nprint(f\"   Validation Macro F1: {best_model_row['Val_Macro_F1']:.4f}\")\nprint(f\"   Validation Accuracy: {best_model_row['Val_Accuracy']:.4f}\")\n\nprint(f\"\\nüìÅ SAVED ARTIFACTS:\")\nprint(f\"   ‚Ä¢ {len(tuned_models)} trained models\")\nprint(f\"   ‚Ä¢ Scaler and imputer\")\nprint(f\"   ‚Ä¢ {len(important_features)} selected features\")\nprint(f\"   ‚Ä¢ Best hyperparameters\")\nprint(f\"   ‚Ä¢ Cross-validation results\")\nprint(f\"   ‚Ä¢ Holdout test data (RESERVED)\")\n\nprint(f\"\\nüî¨ RESEARCH CONTRIBUTIONS:\")\nprint(f\"   ‚úì Hierarchical ML approach (2 stages)\")\nprint(f\"   ‚úì Clinical rule-based labeling\")\nprint(f\"   ‚úì Feature engineering (APRI, FIB-4, AST/ALT)\")\nprint(f\"   ‚úì Feature selection (Mutual Information)\")\nprint(f\"   ‚úì Class imbalance handling (ADASYN)\")\nprint(f\"   ‚úì Hyperparameter optimization\")\nprint(f\"   ‚úì Stratified cross-validation\")\nprint(f\"   ‚úì Explainable AI (SHAP)\")\nprint(f\"   ‚úì Reserved holdout test for unbiased evaluation\")\n\nprint(f\"\\n‚ö†Ô∏è  IMPORTANT FOR EXAMINER DEMONSTRATION:\")\nprint(f\"   ‚Ä¢ Holdout test set: {len(X_final_test):,} samples (UNTOUCHED)\")\nprint(f\"   ‚Ä¢ File: 'holdout_test_data_RESERVED.pkl'\")\nprint(f\"   ‚Ä¢ Ready for final unbiased testing\")\n\nprint(f\"\\n\" + \"=\"*100)\nprint(f\"Completed at: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\nprint(\"=\"*100)\n\n# # ============================================================================\n# # QUICK TEST FUNCTION FOR EXAMINER (OPTIONAL)\n# # ============================================================================\ndef quick_test_on_holdout():\n    \"\"\"\n    Function to quickly test best model on holdout set\n    USE ONLY DURING EXAMINER DEMONSTRATION\n    \"\"\"\n    print(\"\\n\" + \"=\"*100)\n    print(\"‚ö†Ô∏è FINAL HOLDOUT TEST - FOR EXAMINER ONLY\")\n    print(\"=\"*100)\n    \n    # Load holdout data\n    with open('holdout_test_data_RESERVED.pkl', 'rb') as f:\n        holdout_data = pickle.load(f)\n    \n    X_holdout = holdout_data['X_holdout']\n    y_holdout = holdout_data['y_holdout']\n    \n    print(f\"‚úì Loaded holdout set: {len(X_holdout):,} samples\")\n    print(f\"‚úì Created at: {holdout_data['created_at']}\")\n    \n    # Test best model (FIXED: evaluate_model takes 3 arguments, not 4)\n    holdout_results = evaluate_model(\n        best_model, \n        X_holdout, \n        y_holdout, \n        f\"{best_model_name} - HOLDOUT TEST\"\n    )\n    \n    print(f\"\\nüèÜ FINAL HOLDOUT TEST RESULTS:\")\n    print(f\"   Accuracy:    {holdout_results['accuracy']:.4f}\")\n    print(f\"   Macro F1:    {holdout_results['macro_f1']:.4f}\")\n    print(f\"   Weighted F1: {holdout_results['weighted_f1']:.4f}\")\n    if holdout_results['roc_auc']:\n        print(f\"   ROC-AUC:     {holdout_results['roc_auc']:.4f}\")\n    \n    # Create holdout confusion matrix visualization\n    import matplotlib.pyplot as plt\n    import seaborn as sns\n    \n    fig, ax = plt.subplots(figsize=(10, 8))\n    cm = holdout_results['confusion_matrix']\n    \n    sns.heatmap(cm, annot=True, fmt='d', cmap='YlOrRd', cbar=True,\n                xticklabels=['Steatosis', 'NASH', 'Fibrosis', 'Cirrhosis'],\n                yticklabels=['Steatosis', 'NASH', 'Fibrosis', 'Cirrhosis'],\n                linewidths=2, linecolor='white', ax=ax,\n                annot_kws={'fontsize': 14, 'fontweight': 'bold'})\n    \n    ax.set_xlabel('Predicted Severity', fontsize=13, fontweight='bold')\n    ax.set_ylabel('True Severity', fontsize=13, fontweight='bold')\n    ax.set_title(f'HOLDOUT TEST - Confusion Matrix\\n{best_model_name}', \n                 fontsize=15, fontweight='bold', pad=20)\n    \n    plt.tight_layout()\n    plt.savefig('HOLDOUT_confusion_matrix.png', dpi=300, bbox_inches='tight')\n    plt.close()\n    \n    print(f\"\\n‚úì Saved: HOLDOUT_confusion_matrix.png\")\n    \n    return holdout_results\n\n\n \n\n    ","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\"\"\"\nEnhanced NAFLD Severity Classification with Comprehensive Visualizations\n=========================================================================\nAdds: Professional plots, confusion matrices, ROC curves, SHAP analysis\nAuthor: Enhanced Pipeline v3.0\nDate: 2025-12-04\n\"\"\"\n\nimport pandas as pd\nimport numpy as np\nimport pickle\nimport warnings\nfrom datetime import datetime\nimport time\n\n# Machine Learning\nfrom sklearn.model_selection import train_test_split, RandomizedSearchCV, cross_val_score, StratifiedKFold\nfrom sklearn.preprocessing import StandardScaler, label_binarize\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.feature_selection import mutual_info_classif\nfrom xgboost import XGBClassifier\nfrom imblearn.over_sampling import SMOTE\n# Metrics\nfrom sklearn.metrics import (\n    accuracy_score, f1_score, classification_report, confusion_matrix,\n    roc_auc_score, roc_curve, auc, make_scorer, precision_recall_curve\n)\n\n# Visualization\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom matplotlib.gridspec import GridSpec\n\n# SHAP\ntry:\n    import shap\n    SHAP_AVAILABLE = True\nexcept ImportError:\n    SHAP_AVAILABLE = False\n    print(\"‚ö†Ô∏è SHAP not installed. Install with: pip install shap\")\n\nwarnings.filterwarnings('ignore')\n\n# Set style for professional plots\nplt.style.use('seaborn-v0_8-darkgrid')\nsns.set_palette(\"husl\")\n\n# ============================================================================\n# VISUALIZATION FUNCTIONS\n# ============================================================================\n\ndef plot_data_distribution(df, output_dir='plots'):\n    \"\"\"Plot comprehensive data distribution analysis\"\"\"\n    import os\n    os.makedirs(output_dir, exist_ok=True)\n    \n    fig = plt.figure(figsize=(20, 12))\n    gs = GridSpec(3, 4, figure=fig, hspace=0.3, wspace=0.3)\n    \n    # 1. Severity Class Distribution\n    ax1 = fig.add_subplot(gs[0, :2])\n    severity_counts = df['Severity'].value_counts().sort_index()\n    severity_names = ['Simple Steatosis', 'NASH', 'Fibrosis', 'Cirrhosis']\n    colors = ['#2ecc71', '#f39c12', '#e74c3c', '#8e44ad']\n    \n    bars = ax1.bar(range(len(severity_counts)), severity_counts.values, color=colors, alpha=0.7, edgecolor='black')\n    ax1.set_xticks(range(len(severity_counts)))\n    ax1.set_xticklabels(severity_names, rotation=45, ha='right')\n    ax1.set_ylabel('Count', fontsize=12, fontweight='bold')\n    ax1.set_title('NAFLD Severity Distribution', fontsize=14, fontweight='bold')\n    ax1.grid(axis='y', alpha=0.3)\n    \n    # Add count labels on bars\n    for bar, count in zip(bars, severity_counts.values):\n        height = bar.get_height()\n        ax1.text(bar.get_x() + bar.get_width()/2., height,\n                f'{count:,}\\n({count/len(df)*100:.1f}%)',\n                ha='center', va='bottom', fontsize=10, fontweight='bold')\n    \n    # 2. Age Distribution by Severity\n    ax2 = fig.add_subplot(gs[0, 2:])\n    for severity, name, color in zip(range(4), severity_names, colors):\n        data = df[df['Severity'] == severity]['Age']\n        ax2.hist(data, bins=30, alpha=0.5, label=name, color=color, edgecolor='black')\n    ax2.set_xlabel('Age (years)', fontsize=12, fontweight='bold')\n    ax2.set_ylabel('Frequency', fontsize=12, fontweight='bold')\n    ax2.set_title('Age Distribution by Severity', fontsize=14, fontweight='bold')\n    ax2.legend()\n    ax2.grid(alpha=0.3)\n    \n    # 3. BMI Distribution\n    ax3 = fig.add_subplot(gs[1, 0])\n    ax3.boxplot([df[df['Severity']==i]['BMI'].dropna() for i in range(4)],\n                labels=severity_names, patch_artist=True,\n                boxprops=dict(facecolor='lightblue', alpha=0.7))\n    ax3.set_ylabel('BMI', fontsize=12, fontweight='bold')\n    ax3.set_title('BMI by Severity', fontsize=12, fontweight='bold')\n    ax3.tick_params(axis='x', rotation=45)\n    ax3.grid(axis='y', alpha=0.3)\n    \n    # 4. Platelets Distribution\n    ax4 = fig.add_subplot(gs[1, 1])\n    ax4.boxplot([df[df['Severity']==i]['Platelets_k'].dropna() for i in range(4)],\n                labels=severity_names, patch_artist=True,\n                boxprops=dict(facecolor='lightcoral', alpha=0.7))\n    ax4.set_ylabel('Platelets (√ó10¬≥/¬µL)', fontsize=12, fontweight='bold')\n    ax4.set_title('Platelet Count by Severity', fontsize=12, fontweight='bold')\n    ax4.tick_params(axis='x', rotation=45)\n    ax4.grid(axis='y', alpha=0.3)\n    \n    # 5. AST Distribution\n    ax5 = fig.add_subplot(gs[1, 2])\n    ax5.violinplot([df[df['Severity']==i]['AST'].dropna() for i in range(4)],\n                   positions=range(4), showmeans=True)\n    ax5.set_xticks(range(4))\n    ax5.set_xticklabels(severity_names, rotation=45, ha='right')\n    ax5.set_ylabel('AST (U/L)', fontsize=12, fontweight='bold')\n    ax5.set_title('AST by Severity', fontsize=12, fontweight='bold')\n    ax5.grid(axis='y', alpha=0.3)\n    \n    # 6. ALT Distribution\n    ax6 = fig.add_subplot(gs[1, 3])\n    ax6.violinplot([df[df['Severity']==i]['ALT'].dropna() for i in range(4)],\n                   positions=range(4), showmeans=True)\n    ax6.set_xticks(range(4))\n    ax6.set_xticklabels(severity_names, rotation=45, ha='right')\n    ax6.set_ylabel('ALT (U/L)', fontsize=12, fontweight='bold')\n    ax6.set_title('ALT by Severity', fontsize=12, fontweight='bold')\n    ax6.grid(axis='y', alpha=0.3)\n    # 7. Biomarker Correlation Heatmap\n    ax7 = fig.add_subplot(gs[2, :2])\n    biomarkers = ['AST', 'ALT', 'GGT', 'Platelets_k', 'BMI', 'Age', 'FIB4', 'APRI']\n    corr_matrix = df[biomarkers].corr()\n    sns.heatmap(corr_matrix, annot=True, fmt='.2f', cmap='coolwarm', center=0,\n                ax=ax7, square=True, linewidths=1, cbar_kws={'label': 'Correlation'})\n    ax7.set_title('Biomarker Correlation Matrix', fontsize=14, fontweight='bold')\n    \n    # 8. FIB4 Score Distribution\n    ax8 = fig.add_subplot(gs[2, 2:])\n    for severity, name, color in zip(range(4), severity_names, colors):\n        data = df[df['Severity'] == severity]['FIB4']\n        ax8.hist(data, bins=30, alpha=0.5, label=name, color=color, edgecolor='black')\n    ax8.set_xlabel('FIB-4 Score', fontsize=12, fontweight='bold')\n    ax8.set_ylabel('Frequency', fontsize=12, fontweight='bold')\n    ax8.set_title('FIB-4 Score Distribution by Severity', fontsize=14, fontweight='bold')\n    ax8.legend()\n    ax8.grid(alpha=0.3)\n    ax8.set_xlim(0, 5)\n    \n    plt.suptitle('NAFLD Dataset: Comprehensive Distribution Analysis', \n                 fontsize=16, fontweight='bold', y=0.995)\n    plt.savefig(f'{output_dir}/01_data_distribution.png', dpi=300, bbox_inches='tight')\n    plt.close()\n    print(f\"‚úì Saved: {output_dir}/01_data_distribution.png\")\ndef plot_confusion_matrices(results, output_dir='plots'):\n    \"\"\"Plot confusion matrices for all models\"\"\"\n    import os\n    os.makedirs(output_dir, exist_ok=True)\n    \n    n_models = len(results)\n    fig, axes = plt.subplots(2, 2, figsize=(16, 14))\n    axes = axes.flatten()\n    \n    severity_names = ['Steatosis', 'NASH', 'Fibrosis', 'Cirrhosis']\n    \n    for idx, (model_name, result) in enumerate(results.items()):\n        cm = result['confusion_matrix']\n        \n        # Normalize confusion matrix\n        cm_normalized = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n        \n        # Plot\n        ax = axes[idx]\n        im = ax.imshow(cm_normalized, interpolation='nearest', cmap='Blues')\n        ax.set_title(f'{model_name}\\nAccuracy: {result[\"accuracy\"]:.4f}', \n                     fontsize=14, fontweight='bold')\n        \n        # Colorbar\n        cbar = plt.colorbar(im, ax=ax, fraction=0.046, pad=0.04)\n        cbar.set_label('Normalized Count', fontsize=10)\n        # Labels\n        tick_marks = np.arange(len(severity_names))\n        ax.set_xticks(tick_marks)\n        ax.set_yticks(tick_marks)\n        ax.set_xticklabels(severity_names, rotation=45, ha='right')\n        ax.set_yticklabels(severity_names)\n        \n        # Add text annotations\n        thresh = cm_normalized.max() / 2.\n        for i in range(cm.shape[0]):\n            for j in range(cm.shape[1]):\n                ax.text(j, i, f'{cm[i, j]}\\n({cm_normalized[i, j]:.2f})',\n                       ha=\"center\", va=\"center\",\n                       color=\"white\" if cm_normalized[i, j] > thresh else \"black\",\n                       fontsize=10, fontweight='bold')\n        \n        ax.set_ylabel('True Label', fontsize=12, fontweight='bold')\n        ax.set_xlabel('Predicted Label', fontsize=12, fontweight='bold')\n    \n    plt.suptitle('Confusion Matrices - All Models', fontsize=16, fontweight='bold')\n    plt.tight_layout()\n    plt.savefig(f'{output_dir}/02_confusion_matrices.png', dpi=300, bbox_inches='tight')\n    plt.close()\n    print(f\"‚úì Saved: {output_dir}/02_confusion_matrices.png\")\n\n\ndef plot_roc_curves(results, X_test, y_test, models, output_dir='plots'):\n    \"\"\"Plot ROC curves for all models and classes\"\"\"\n    import os\n    os.makedirs(output_dir, exist_ok=True)\n    \n    severity_names = ['Steatosis', 'NASH', 'Fibrosis', 'Cirrhosis']\n    colors = ['#2ecc71', '#f39c12', '#e74c3c', '#8e44ad']\n    \n    # Binarize labels for One-vs-Rest ROC\n    y_test_bin = label_binarize(y_test, classes=[0, 1, 2, 3])\n    n_classes = y_test_bin.shape[1]\n    \n    fig, axes = plt.subplots(2, 2, figsize=(18, 16))\n    axes = axes.flatten()\n    for idx, (model_name, model)in enumerate(models.items()):\n        ax = axes[idx]\n        \n        # Get predictions\n        y_score = model.predict_proba(X_test)\n        \n        # Compute ROC curve and AUC for each class\n        fpr = dict()\n        tpr = dict()\n        roc_auc = dict()\n        \n        for i in range(n_classes):\n            fpr[i], tpr[i], _ = roc_curve(y_test_bin[:, i], y_score[:, i])\n            roc_auc[i] = auc(fpr[i], tpr[i])\n        \n        # Plot ROC curves\n        for i, color, name in zip(range(n_classes), colors, severity_names):\n            ax.plot(fpr[i], tpr[i], color=color, lw=2.5,\n                   label=f'{name} (AUC = {roc_auc[i]:.3f})')\n        \n        # Plot diagonal\n        ax.plot([0, 1], [0, 1], 'k--', lw=2, alpha=0.3, label='Random Classifier')\n        \n        ax.set_xlim([0.0, 1.0])\n        ax.set_ylim([0.0, 1.05])\n        ax.set_xlabel('False Positive Rate', fontsize=12, fontweight='bold')\n        ax.set_ylabel('True Positive Rate', fontsize=12, fontweight='bold')\n        # Calculate macro-average AUC\n        macro_auc = np.mean(list(roc_auc.values()))\n        ax.set_title(f'{model_name}\\nMacro-Avg AUC = {macro_auc:.3f}', \n                     fontsize=14, fontweight='bold')\n        ax.legend(loc=\"lower right\", fontsize=10)\n        ax.grid(alpha=0.3)\n    \n    plt.suptitle('ROC Curves - Multiclass One-vs-Rest', fontsize=16, fontweight='bold')\n    plt.tight_layout()\n    plt.savefig(f'{output_dir}/03_roc_curves.png', dpi=300, bbox_inches='tight')\n    plt.close()\n    print(f\"‚úì Saved: {output_dir}/03_roc_curves.png\")\n\n\ndef plot_precision_recall_curves(results, X_test, y_test, models, output_dir='plots'):\n    \"\"\"Plot Precision-Recall curves for all models\"\"\"\n    import os\n    os.makedirs(output_dir, exist_ok=True)\n    \n    severity_names = ['Steatosis', 'NASH', 'Fibrosis', 'Cirrhosis']\n    colors = ['#2ecc71', '#f39c12', '#e74c3c', '#8e44ad']\n    \n    y_test_bin = label_binarize(y_test, classes=[0, 1, 2, 3])\n    n_classes = y_test_bin.shape[1]\n    \n    fig, axes = plt.subplots(2, 2, figsize=(18, 16))\n    \n    axes = axes.flatten()\n    \n    for idx, (model_name, model) in enumerate(models.items()):\n        ax = axes[idx]\n        y_score = model.predict_proba(X_test)\n        \n        for i, color, name in zip(range(n_classes), colors, severity_names):\n            precision, recall, _ = precision_recall_curve(y_test_bin[:, i], y_score[:, i])\n            ap = auc(recall, precision)\n            ax.plot(recall, precision, color=color, lw=2.5,\n                   label=f'{name} (AP = {ap:.3f})')\n        \n        ax.set_xlim([0.0, 1.0])\n        ax.set_ylim([0.0, 1.05])\n        ax.set_xlabel('Recall', fontsize=12, fontweight='bold')\n        ax.set_ylabel('Precision', fontsize=12, fontweight='bold')\n        ax.set_title(f'{model_name} - Precision-Recall Curves', fontsize=14, fontweight='bold')\n        ax.legend(loc=\"lower left\", fontsize=10)\n        ax.grid(alpha=0.3)\n    \n    plt.suptitle('Precision-Recall Curves - All Models', fontsize=16, fontweight='bold')\n    plt.tight_layout()\n    plt.savefig(f'{output_dir}/04_precision_recall_curves.png', dpi=300, bbox_inches='tight')\n    plt.close()\n    print(f\"‚úì Saved: {output_dir}/04_precision_recall_curves.png\")\ndef plot_model_comparison(comparison_df, cv_results, output_dir='plots'):\n    \"\"\"Plot comprehensive model comparison\"\"\"\n    import os\n    os.makedirs(output_dir, exist_ok=True)\n    \n    fig = plt.figure(figsize=(18, 10))\n    gs = GridSpec(2, 3, figure=fig, hspace=0.3, wspace=0.3)\n    \n    models = comparison_df['Model'].tolist()\n    colors = ['#3498db', '#e74c3c', '#2ecc71', '#f39c12']\n    \n    # 1. Macro F1 Comparison (CV vs Validation)\n    ax1 = fig.add_subplot(gs[0, 0])\n    x = np.arange(len(models))\n    width = 0.35\n    \n    cv_f1 = comparison_df['CV_F1_Mean'].values\n    val_f1 = comparison_df['Val_Macro_F1'].values\n    \n    bars1 = ax1.bar(x - width/2, cv_f1, width, label='CV Macro F1', \n                    color='steelblue', alpha=0.8, edgecolor='black')\n    bars2 = ax1.bar(x + width/2, val_f1, width, label='Val Macro F1',\n                    color='coral', alpha=0.8, edgecolor='black')\n    \n    ax1.set_xlabel('Model', fontsize=12, fontweight='bold')\n    ax1.set_ylabel('Macro F1 Score', fontsize=12, fontweight='bold')\n    ax1.set_title('Cross-Validation vs Validation F1', fontsize=14, fontweight='bold')\n    ax1.set_xticks(x)\n    ax1.set_xticklabels(models, rotation=45, ha='right')\n    ax1.legend()\n    ax1.grid(axis='y', alpha=0.3)\n    ax1.set_ylim([0, 1.05])\n    \n    # Add value labels\n    for bars in [bars1, bars2]:\n        for bar in bars:\n            height = bar.get_height()\n            ax1.text(bar.get_x() + bar.get_width()/2., height,\n                    f'{height:.3f}', ha='center', va='bottom', fontsize=9)\n    \n    # 2. Multiple Metrics Comparison\n    ax2 = fig.add_subplot(gs[0, 1])\n    metrics = ['Val_Accuracy', 'Val_Macro_F1', 'Val_Weighted_F1', 'Val_ROC_AUC']\n    metric_labels = ['Accuracy', 'Macro F1', 'Weighted F1', 'ROC-AUC']\n    \n    x = np.arange(len(models))\n    width = 0.2\n    \n    for i, (metric, label) in enumerate(zip(metrics, metric_labels)):\n        values = comparison_df[metric].values\n        ax2.bar(x + i*width, values, width, label=label, alpha=0.8, edgecolor='black')\n    \n    ax2.set_xlabel('Model', fontsize=12, fontweight='bold')\n    ax2.set_ylabel('Score', fontsize=12, fontweight='bold')\n    ax2.set_title('Multi-Metric Model Comparison', fontsize=14, fontweight='bold')\n    ax2.set_xticks(x + width * 1.5)\n    ax2.set_xticklabels(models, rotation=45, ha='right')\n    ax2.legend(loc='lower right')\n    ax2.grid(axis='y', alpha=0.3)\n    ax2.set_ylim([0, 1.05])\n    \n    # 3. Cross-Validation Stability\n    ax3 = fig.add_subplot(gs[0, 2])\n    cv_means = [cv_results[m]['mean'] for m in models]\n    cv_stds = [cv_results[m]['std'] for m in models]\n    \n    bars = ax3.bar(range(len(models)), cv_means, yerr=cv_stds,\n                   color=colors, alpha=0.7, capsize=5, edgecolor='black')\n    ax3.set_xlabel('Model', fontsize=12, fontweight='bold')\n    ax3.set_ylabel('CV Macro F1', fontsize=12, fontweight='bold')\n    ax3.set_title('Cross-Validation Stability', fontsize=14, fontweight='bold')\n    ax3.set_xticks(range(len(models)))\n    ax3.set_xticklabels(models, rotation=45, ha='right')\n    ax3.grid(axis='y', alpha=0.3)\n    \n    for bar, mean, std in zip(bars, cv_means, cv_stds):\n        height = bar.get_height()\n        ax3.text(bar.get_x() + bar.get_width()/2., height,\n                f'{mean:.4f}\\n¬±{std:.4f}', ha='center', va='bottom', fontsize=9)\n    \n    # 4. Model Ranking Heatmap\n    ax4 = fig.add_subplot(gs[1, :])\n    \n    ranking_data = comparison_df[['Model', 'CV_F1_Mean', 'Val_Accuracy', \n                                   'Val_Macro_F1', 'Val_Weighted_F1', 'Val_ROC_AUC']].copy()\n    ranking_data = ranking_data.set_index('Model')\n    \n    # Normalize to 0-1 scale for heatmap\n    ranking_normalized = (ranking_data - ranking_data.min()) / (ranking_data.max() - ranking_data.min())\n    \n    sns.heatmap(ranking_normalized.T, annot=ranking_data.T, fmt='.4f', \n                cmap='RdYlGn', center=0.5, ax=ax4, cbar_kws={'label': 'Normalized Score'},\n                linewidths=1, linecolor='black')\n    ax4.set_title('Model Performance Heatmap (All Metrics)', fontsize=14, fontweight='bold')\n    ax4.set_xlabel('Model', fontsize=12, fontweight='bold')\n    ax4.set_ylabel('Metric', fontsize=12, fontweight='bold')\n    plt.suptitle('Comprehensive Model Comparison Dashboard', fontsize=16, fontweight='bold')\n    plt.savefig(f'{output_dir}/05_model_comparison.png', dpi=300, bbox_inches='tight')\n    plt.close()\n    print(f\"‚úì Saved: {output_dir}/05_model_comparison.png\")\n\n\ndef plot_feature_importance_comparison(feature_importance_dfs, output_dir='plots'):\n    \"\"\"Compare feature importance across models\"\"\"\n    import os\n    os.makedirs(output_dir, exist_ok=True)\n    \n    fig, axes = plt.subplots(2, 2, figsize=(18, 14))\n    axes = axes.flatten()\n    \n    for idx, (model_name, df) in enumerate(feature_importance_dfs.items()):\n        ax = axes[idx]\n        \n        # Get top 10 features\n        top_features = df.head(10).copy()\n        \n        # Determine column name\n        value_col = 'Importance' if 'Importance' in top_features.columns else 'Coefficient'\n        \n        # Plot horizontal bar chart\n        y_pos = np.arange(len(top_features))\n        colors_gradient = plt.cm.viridis(np.linspace(0.3, 0.9, len(top_features)))\n        \n        bars = ax.barh(y_pos, top_features[value_col].values, \n                      color=colors_gradient, edgecolor='black', alpha=0.8)\n        \n        ax.set_yticks(y_pos)\n        ax.set_yticklabels(top_features['Feature'].values)\n        ax.invert_yaxis()\n        ax.set_xlabel(value_col, fontsize=12, fontweight='bold')\n        ax.set_title(f'{model_name} - Top 10 Features', fontsize=14, fontweight='bold')\n        ax.grid(axis='x', alpha=0.3)\n        \n        # Add value labels\n        for i, (bar, val) in enumerate(zip(bars, top_features[value_col].values)):\n            ax.text(val, i, f' {val:.4f}', va='center', fontsize=9, fontweight='bold')\n    \n    plt.suptitle('Feature Importance Comparison Across Models', fontsize=16, fontweight='bold')\n    plt.tight_layout()\n    plt.savefig(f'{output_dir}/06_feature_importance_comparison.png', dpi=300, bbox_inches='tight')\n    plt.close()\n    print(f\"‚úì Saved: {output_dir}/06_feature_importance_comparison.png\")\n\n\ndef plot_shap_analysis(model, X_sample, feature_names, model_name, output_dir='plots'):\n    \"\"\"Comprehensive SHAP analysis plots\"\"\"\n    import os\n    os.makedirs(output_dir, exist_ok=True)\n    \n    if not SHAP_AVAILABLE:\n        print(\"‚ö†Ô∏è SHAP not available, skipping SHAP plots\")\n        return\n        \n    if len(X_sample) > sample_size:\n         X_sample_small = X_sample.sample(sample_size, random_state=42)\n    else:\n        X_sample_small = X_sample.copy()\n    \n    try:\n        print(f\"\\nüîç Generating SHAP plots for {model_name}...\")\n        \n        # Create explainer\n        explainer = shap.TreeExplainer(model)\n        shap_values = explainer.shap_values(X_sample)\n        \n        # Convert to DataFrame for easier handling\n        X_sample_df = pd.DataFrame(X_sample, columns=feature_names)\n        \n        # 1. Summary Plot (Bar)\n        plt.figure(figsize=(12, 8))\n        if isinstance(shap_values, list):\n            shap.summary_plot(shap_values, X_sample_df, plot_type=\"bar\", show=False, max_display=15)\n        else:\n            shap.summary_plot(shap_values, X_sample_df, plot_type=\"bar\", show=False, max_display=15)\n        plt.title(f'SHAP Feature Importance - {model_name}', fontsize=14, fontweight='bold', pad=20)\n        plt.tight_layout()\n        plt.savefig(f'{output_dir}/07_shap_summary_bar_{model_name.replace(\" \", \"_\")}.png', \n                    dpi=300, bbox_inches='tight')\n        plt.close()\n        print(f\"‚úì Saved: SHAP summary bar plot\")\n        \n        # 2. Summary Plot (Beeswarm) - shows feature impact\n        plt.figure(figsize=(12, 10))\n        if isinstance(shap_values, list):\n               # For multiclass, show all classes\n            shap.summary_plot(shap_values, X_sample_df, show=False, max_display=15)\n        else:\n            shap.summary_plot(shap_values, X_sample_df, show=False, max_display=15)\n        plt.title(f'SHAP Feature Impact - {model_name}', fontsize=14, fontweight='bold', pad=20)\n        plt.tight_layout()\n        plt.savefig(f'{output_dir}/08_shap_beeswarm_{model_name.replace(\" \", \"_\")}.png', \n                    dpi=300, bbox_inches='tight')\n        plt.close()\n        print(f\"‚úì Saved: SHAP beeswarm plot\")\n        \n        # 3. SHAP Dependence Plots for top 3 features\n        if isinstance(shap_values, list):\n            shap_values_class0 = shap_values[0]\n        else:\n            shap_values_class0 = shap_values\n        \n        # Calculate mean absolute SHAP values\n        mean_abs_shap = np.abs(shap_values_class0).mean(axis=0)\n        top_features_idx = np.argsort(mean_abs_shap)[-3:][::-1]\n        \n        fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n        for idx, feature_idx in enumerate(top_features_idx):\n            feature_name = feature_names[feature_idx]\n            shap.dependence_plot(feature_idx, shap_values_class0, X_sample_df, \n                               ax=axes[idx], show=False)\n            axes[idx].set_title(f'SHAP Dependence: {feature_name}', \n                              fontsize=12, fontweight='bold')\n        \n        plt.suptitle(f'SHAP Dependence Plots - Top 3 Features ({model_name})', \n                    fontsize=14, fontweight='bold')\n        plt.tight_layout()\n        plt.savefig(f'{output_dir}/09_shap_dependence_{model_name.replace(\" \", \"_\")}.png', \n                    dpi=300, bbox_inches='tight')\n        plt.close()\n        print(f\"‚úì Saved: SHAP dependence plots\")\n        \n        # 4. SHAP Force Plot for a single prediction (saved as image)\n        plt.figure(figsize=(20, 3))\n        if isinstance(shap_values, list):\n            shap.force_plot(explainer.expected_value[0], shap_values[0][0], \n                          X_sample_df.iloc[0], matplotlib=True, show=False)\n        else:\n            shap.force_plot(explainer.expected_value, shap_values[0], \n                          X_sample_df.iloc[0], matplotlib=True, show=False)\n        plt.title(f'SHAP Force Plot - Single Prediction ({model_name})', \n                 fontsize=14, fontweight='bold', pad=20)\n        plt.tight_layout()\n        plt.savefig(f'{output_dir}/10_shap_force_{model_name.replace(\" \", \"_\")}.png', dpi=300, bbox_inches='tight')\n        plt.close()\n        print(f\"‚úì Saved: SHAP force plot\")\n    except Exception as e:\n        print(f\"‚ö†Ô∏è Error generating SHAP plots for {model_name}: {e}\")\n\n         ","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}